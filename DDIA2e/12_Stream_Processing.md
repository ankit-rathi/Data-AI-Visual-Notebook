# RIGHT PAGE â€” Article

## Stream Processing

Batch processing assumes the world pauses long enough for us to analyze it. Stream processing assumes it never does.

In batch systems, data is treated as complete. A dataset arrives, we process it, and we produce an output. There is a clear beginning and end. But most real-world systems do not behave this way. Users keep clicking. Sensors keep emitting readings. Markets keep moving. Data is not finite â€” it is unbounded.

Stream processing is a shift in mindset. Instead of waiting for data to be complete, we process each event as it arrives. An event is the atomic unit of a stream: a small, immutable record of something that happened at a specific time. The system never finishes; it continuously incorporates new events into its state.

This changes how we think about infrastructure. Traditional message brokers often treat messages as disposable. Once acknowledged, they disappear. But log-based systems treat streams as append-only logs. Nothing is erased. Consumers track their own position. This allows replaying history, rebuilding state, and deriving new views from past events.

At a deeper level, stream processing reveals a powerful duality: state and streams are two perspectives on the same reality. Current state can be seen as the accumulation of all past events. Conversely, a stream of changes is simply the record of how state evolves over time. Techniques like Change Data Capture make this explicit by turning database mutations into event streams.

Time becomes subtle. Events have an event time â€” when they actually happened â€” and a processing time â€” when the system handled them. If these diverge, naive windowing can produce misleading results. Stream processors must reason carefully about windows, joins, and late arrivals.

Fault tolerance also changes. Because streams are infinite, you cannot restart from the beginning. Instead, systems rely on checkpointing, idempotence, and transactional guarantees to achieve exactly-once semantics.

Stream processing is not just faster batch processing. It is a different mental model: the world as a continuous flow of events, where systems must remain correct while the story is still unfolding.

---

# LEFT PAGE â€” Visual Note

## Stream Processing

**Computing while the story unfolds**

---

### 1ï¸âƒ£ Event as the Atom

ğŸ§© Icon: small dot or spark

* Small immutable record (cannot be changed after creation â€” like ink on paper)
* Something happened (a click, payment, reading â€” concrete action)
* Has a timestamp (anchors it in time â€” narrative marker)
* Self-contained fact (no hidden context â€” portable truth)

---

### 2ï¸âƒ£ Batch vs Stream

ğŸ“¦â¡ï¸ğŸŒŠ Icon: box turning into wave

* Batch = finite dataset (clear start and end â€” closed book)
* Stream = unbounded flow (never-ending input â€” live broadcast)
* Batch waits for completeness (delayed reflection â€” report tomorrow)
* Stream reacts immediately (near-real-time response â€” live dashboard)

---

### 3ï¸âƒ£ Messaging Evolution

ğŸ“¨ Icon: envelope vs log stack

* AMQP/JMS: message deleted after ack (task queue model â€” disposable mail)
* Order less critical (parallel tasks â€” factory line)
* Log-based broker: append-only log (like a ledger â€” permanent record)
* Replay possible (rebuild state â€” rewind history)

---

### 4ï¸âƒ£ State â†” Stream Duality

â™»ï¸ Icon: loop arrow

* State = accumulation of events (sum over time â€” running total)
* Stream = record of changes (diff of state â€” change history)
* Change Data Capture (database emits updates â€” shadow narrator)
* Derived systems stay synced (search, cache, analytics â€” mirrors)

---

### 5ï¸âƒ£ Time Complexity

â³ Icon: clock split in two

* Event time (when it actually happened â€” story order)
* Processing time (when system handled it â€” viewing order)
* Late arrivals distort windows (network lag â€” delayed scenes)
* Windows define analysis scope (tumbling, sliding, session â€” framing lens)

---

### 6ï¸âƒ£ Stream Joins

ğŸ”— Icon: linking chains

* Streamâ€“stream join (match events within window â€” search â†’ click)
* Streamâ€“table join (enrich with database â€” add profile info)
* Tableâ€“table join (combine changelogs â€” materialized view)
* Continuous matching (never fully done â€” moving target)

---

### 7ï¸âƒ£ Fault Tolerance

ğŸ›¡ï¸ Icon: shield

* Streams are infinite (cannot restart from zero â€” no rewind to origin)
* Checkpoint state (periodic snapshot â€” save progress)
* Idempotent writes (safe repetition â€” pressing button twice same effect)
* Exactly-once semantics (effect as if once â€” illusion of perfection)

---

# YOUTUBE SHORTS â€” 60s Transcript

When we talk about data processing, we often imagine a dataset that arrives, gets processed, and produces an output. Thatâ€™s batch thinking. But most systems donâ€™t operate in batches. They operate in streams.

Users keep clicking. Sensors keep emitting signals. Markets keep updating. Data doesnâ€™t stop. It flows.

Stream processing is about handling each event as it happens. An event is a small, immutable fact â€” something that occurred at a specific time. The system continuously updates its state as new events arrive.

Whatâ€™s powerful is the duality between state and streams. Your current database state is just the accumulation of all past events. And a stream is simply the record of how that state changes over time.

But time becomes tricky. Thereâ€™s when an event actually happened, and when your system processed it. If those donâ€™t align, your analytics can mislead you.

So stream processing isnâ€™t just about speed. Itâ€™s about reasoning correctly in a world that never stops generating events.
