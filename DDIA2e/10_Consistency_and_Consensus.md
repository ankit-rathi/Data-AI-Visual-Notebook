# Consistency and Consensus

*Many nodes. One truth.*

---

## RIGHT PAGE â€” Article (~370 words)

Distributed systems are built from unreliable components. Machines crash. Networks delay. Messages reorder. Yet some systems provide a powerful illusion: that there is only one copy of the data and every operation happens atomically. That illusion is called **linearizability**.

Linearizability guarantees recency. Once a write completes, every subsequent read must see it. The system behaves like a single-threaded variable. This is stronger than serializability, which is about transaction isolation across multiple objects. Linearizability concerns the freshness of individual objects.

The cost is unavoidable. To ensure a value is the latest, nodes must coordinate. Response time becomes proportional to network uncertainty. Strong consistency is slow because doubt must be eliminated.

Ordering lies at the center of this problem. Physical clocks drift, so distributed systems rely on logical clocks. Lamport timestamps preserve causal order: if event A happened before B, A gets a lower timestamp. But causality is weaker than linearizability. Two nodes can generate timestamps without knowing which write is truly the latest globally. Hybrid logical clocks combine physical time with logical counters to approximate real-time ordering while preserving causality.

To implement true fault-tolerant linearizability, systems use **consensus**. Consensus means multiple nodes agree on a single sequence of decisions. If they agree on the order of operations, the system behaves as if one machine executed them.

Algorithms such as Raft and Paxos rely on epochs and quorums. A majority must confirm each decision. This prevents split brain: two leaders accepting conflicting writes. Coordination services like Apache ZooKeeper and etcd package these guarantees into reusable infrastructure for locks, leases, and configuration management.

Many problems reduce to consensus: shared logs, uniqueness constraints, compare-and-set operations, distributed locks, atomic commit. Solve one, and you can transform the solution to others.

However, consensus trades availability for consistency during network partitions. Under the CAP trade-off, systems may choose to reject requests rather than risk stale data.

Consensus is powerful but expensive. It is justified when correctness demands a single authoritative order. Otherwise, weaker consistency may be the wiser design choice.

---

## LEFT PAGE â€” Visual Note (Hand-Drawable)

**Consistency & Consensus**
*Agreement under uncertainty.*

---

### 1ï¸âƒ£ Linearizability

ğŸ“ Icon: Single straight line

* Appears as one copy â€” atomic illusion
* Recency guarantee â€” latest write visible
* Stronger freshness than isolation â€” per-object focus

---

### 2ï¸âƒ£ Cost of Strong Consistency

ğŸ¢ Icon: Slow clock

* Must coordinate before replying â€” quorum round-trip
* Latency tied to network uncertainty â€” unavoidable delay
* Doubt eliminated before success â€” safety first

---

### 3ï¸âƒ£ Ordering the World

ğŸ•°ï¸ Icon: Clock with counter

* Physical clocks drift â€” unreliable global time
* Lamport clocks preserve causality â€” happened-before
* Causality â‰  linearizability â€” global freshness missing

---

### 4ï¸âƒ£ Hybrid Logical Clocks

ğŸ”— Icon: Linked clock gears

* Combine physical time + logical counter â€” best of both
* Approximate real-time ordering â€” bounded skew
* Used in modern distributed DBs â€” practical compromise

---

### 5ï¸âƒ£ Consensus Core

ğŸ§‘â€âš–ï¸ Icon: Group vote

* Uniform agreement â€” no divergence
* Integrity â€” decide once
* Validity + termination â€” real proposal, eventual decision

---

### 6ï¸âƒ£ Algorithms & Quorums

ğŸ›ï¸ Icon: Majority vote panel

* Raft / Paxos â€” leader-based replication
* Epoch numbers prevent split brain â€” single term leader
* Majority quorum confirms writes â€” overlap guarantee

---

### 7ï¸âƒ£ Coordination Services

ğŸ—ï¸ Icon: Key and lock

* Apache ZooKeeper / etcd abstract consensus
* Provide locks, leases, config â€” higher-level tools
* Reuse instead of reimplement â€” reduce risk

---

### 8ï¸âƒ£ CAP Trade-off

âš¡ Icon: Broken network line

* Partition forces choice â€” consistency or availability
* CP returns error to protect truth â€” safe pause
* AP serves stale but live data â€” availability first

---

## YOUTUBE SHORTS (~60 seconds)

Distributed systems are unreliable by nature. Machines fail. Networks delay. Yet some systems behave as if there is only one copy of the data.

That illusion is linearizability. Once a write succeeds, every later read must see it. The system behaves like a single-threaded variable.

But that illusion is expensive. Nodes must coordinate before replying. Latency reflects network uncertainty.

To make this fault-tolerant, systems use consensus. Algorithms like Raft and Paxos ensure that a majority agrees on every operationâ€™s order. Coordination services such as Apache ZooKeeper and etcd provide these guarantees as reusable building blocks.

Many distributed problemsâ€”locks, uniqueness constraints, shared logsâ€”reduce to consensus.

Under network partitions, you must choose: reject requests to preserve consistency, or serve stale data to remain available.

Consensus gives you one truth across many machines.

But it demands patienceâ€”and careful judgment about when that strength is truly necessary.
